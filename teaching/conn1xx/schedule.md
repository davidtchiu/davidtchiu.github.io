# Course materials

## Week 1
### Part 1
    - https://youtu.be/-M6lANfzFsM
    To understand the power and potential of artificial intelligence, it's helpful to look back at the long arc of computing history. Modern computing did not emerge overnight. It is the result of thousands of years of human innovation, beginning with basic tools for arithmetic and evolving into the highly sophisticated technologies that underpin today's digital world. This module introduces key figures, milestones, and technologies that laid the groundwork for computer science and artificial intelligence.

    We begin with the origins of computing in ancient times and trace its transformation through the mechanical, electromechanical, and electronic eras. We'll explore how technological and conceptual breakthroughs like binary arithmetic, programmable machines, and the invention of the transistor set the stage for the computing revolution of the 20th and 21st centuries.

    Watch the first video below for a detailed journey from the abacus to early digital computers. This video also introduces major contributors such as Blaise Pascal, Gottfried Leibniz, Charles Babbage, Ada Lovelace, Alan Turing, Konrad Zuse, and Grace Hopper. As you watch, pay attention to how innovations in hardware (like vacuum tubes and transistors) and software (like programming languages and compilers) worked together to make computing more powerful and accessible.

    After watching the first video, you should have a better understanding of the major eras of computing:

    Mechanical computing (abacus, Pascaline)

    Electromechanical and punch card systems

    Early digital computers and binary logic

    The vacuum tube era

    The emergence of transistors and integrated circuits

    Development of early programming languages like Fortran and COBOL

    To zoom out and see the broader societal implications of these changes, let’s step back further and examine the role of technology in human development more generally. From the wheel and fire to smartphones and artificial intelligence, technological advancement has always been a central driver of how societies function, connect, and evolve.

### Part 2
    Watch the second video below for a higher-level overview of technological progress. It places computing within the larger context of human innovation and shows how developments like the steam engine, the internet, and mobile devices have fundamentally transformed how we live and work.

    - https://youtu.be/YFGNXpErBoY

    This broader view highlights the importance of understanding computing not only as a technical field but also as a societal force. The tools we develop shape how we think, communicate, and solve problems—and artificial intelligence is the latest chapter in that long history.

## Week 2
### Part 1
    In this page, we will explore traditional coding practices to better understand how AI applications are built and how they differ from conventional programming. Coding has long been the foundation of software development, allowing programmers to create structured instructions that computers follow step by step. However, AI introduces a new paradigm—rather than explicitly coding every rule. AI systems can learn patterns and make decisions based on data.

    To introduce you to the core coding concepts, we will use the linked website below, which breaks down coding at a very high level. This website provides an accessible and beginner-friendly explanation, helping you grasp the fundamental ideas behind programming without requiring prior experience.

    As you go through the material, focus on understanding the key takeaways:

    What coding is and why it matters
    How code is structured to give computers instructions
    - https://subjectguides.york.ac.uk/coding/introduction


### Part 2
    - https://youtu.be/zOa5o9Yq_ZU
    In this module, we will take a hands-on approach to understanding traditional coding practices by experimenting with Scratch, a beginner-friendly programming language designed to introduce core computational concepts in a visual and interactive way.

    Scratch uses a block-based coding system, where instead of writing lines of code, you assemble pre-built coding blocks that snap together like puzzle pieces. While this approach simplifies programming, it still reflects many of the fundamental ideas behind traditional coding, such as:

    Sequencing – Executing instructions in a specific order.
    Loops – Repeating actions efficiently.
    Conditional Logic – Making decisions based on different conditions.
    Variables – Storing and manipulating data.
    While AI development often involves working with complex algorithms and large datasets, traditional coding remains at its core. By exploring Scratch, you will gain a basic understanding of how programmers structure instructions and logic, concepts that also play a role in AI development. We will also explore how AI can greatly expand a program's functionality.

    Below, you will find videos and links that introduce the basic functionality of Scratch and guide you through simple programming exercises. As you engage with these materials, think about how these foundational concepts connect to the AI applications we’ve discussed in class.

    Simple Scratch Game: https://youtu.be/D-nW4jvzRr8

## Week 3: What is Artificial Intelligence?
Artificial Intelligence (AI) is a rapidly evolving field, yet there is no single, universally accepted definition of what AI truly is. Different researchers, industries, and organizations define AI based on their perspectives and applications. However, for the purposes of this class, we will define AI as:

    _ "The development of computer systems that can perform tasks typically requiring human intelligence, such as learning, reasoning, problem-solving, perception, and language understanding."_

AI systems can range from simple rule-based programs that follow predefined instructions to complex machine learning models that adapt and improve based on data. While modern AI applications are powerful, they still operate within the constraints of mathematical models, algorithms, and the data they are trained on.

AI is already integrated into many aspects of daily life, from virtual assistants and recommendation systems to self-driving cars and medical diagnostics. As AI continues to advance, discussions around its impact, both positive and negative, are becoming increasingly important.

The video below explores the evolving nature of AI, how it is shaping our world, and the role humanity plays in guiding its future. Watch as Dr. Michael Littman from the U.S. National Science Foundation discusses the challenges and opportunities AI presents.
    - https://youtu.be/HcZ6bq-RVM0

A Brief History of AI
In the video above, Dr. Michael Littman highlighted how AI has long been portrayed in science fiction, from intelligent robots to sentient machines. While these fictional depictions often stretch the limits of reality, the idea of creating machines that can think and learn has fascinated humans for centuries.



AI as a scientific field is relatively new. The foundations of modern AI emerged in the mid-20th century, driven by advancements in mathematics, computing, and cognitive science. Since then, AI has gone through cycles of breakthroughs and setbacks, with periods of rapid progress followed by what researchers call "AI winters"—times when progress slowed due to technological and funding limitations.



In this section, we will explore some of the key moments in AI’s history, from its theoretical roots to today’s widespread applications. Understanding this history will help us see how AI evolved, why it matters, and where it might be headed next.

Watch the videos below to learn about the milestones that have shaped AI into what it is today.
- https://youtu.be/eSj80Zr6TEE


Artificial Intelligence (AI) is a broad and evolving field, and as a result, there are many different ways to classify AI. These classifications often overlap, as AI technologies continue to develop and push the boundaries of what is possible. While some AI systems are designed to perform specific tasks, others are theoretical concepts that represent the future of AI development.


- In this section, we will explore four major levels of AI that help us understand how AI is categorized based on its capabilities

Levels of AI
Weak AI / Narrow AI

AI designed to perform a specific task or set of tasks.
Cannot think or operate beyond its pre-programmed function.
Most AI applications today fall into this category (e.g., chatbots, voice assistants, recommendation systems).
Strong AI / Artificial General Intelligence (AGI)

AI that can perform any intellectual task a human can.
Capable of reasoning, problem-solving, and adapting to new situations without being explicitly programmed.
AGI does not yet exist—it remains a long-term goal of AI research.
General-Purpose AI

A term sometimes used interchangeably with AGI but can also refer to AI models that adapt to multiple tasks without requiring retraining (e.g., large language models like ChatGPT).
Not truly AGI, but more flexible than Narrow AI.
Artificial Superintelligence (ASI)

A hypothetical AI that surpasses human intelligence in every domain.
Would have the ability to improve itself autonomously, leading to rapid advancements beyond human control.
Often discussed in the context of AI ethics and risks, but no such system exists today.
The video below provides an overview of these AI classifications: https://youtu.be/0Jn_A9AZhSc

Debating Artificial General Intelligence: Insights from Davos
Artificial General Intelligence (AGI) is one of the most debated topics in AI today. Unlike Narrow AI, which is designed for specific tasks, AGI refers to an AI system that could reason, learn, and adapt across a broad range of human-like cognitive functions. But is AGI possible? If so, when might it emerge? And should we even pursue it?

- At the World Economic Forum in Davos, leading AI experts and industry figures tackled these questions in a panel discussion that underscored the deep divisions within the AI community. The debate covered three major areas of contention:

Defining AGI: Some argue that AGI is a meaningful goal, while others see intelligence as a growing spectrum rather than a sudden leap.
AGI Risks: Is AGI an existential threat, or just another tool under human control? AI pioneers Andrew Ng and Yoshua Bengio strongly disagreed on this point.
The AI Arms Race: While some advocate slowing AI development for safety, others argue that geopolitical competition—especially between the U.S. and China—makes this unrealistic.
The video below features highlights from the Davos panel, where experts debate the future of AGI, its risks, and its implications for society: https://youtu.be/w5iuHJh3_Gk

## Week 5: ML
Welcome to the next stage of our course: an introduction to machine learning. In this section, we’ll start to explore one of the most impactful areas of modern computing, where computers learn patterns from data and make predictions or decisions without being explicitly programmed for each task.

To get going, we’ll watch a video by Luv Aggarwal, a Data Platform Solution Engineer at IBM. The video will introduce you to what machine learning is, how it differs from related fields like artificial intelligence and deep learning, and the main types of machine learning: supervised, unsupervised, and reinforcement learning. You'll also see brief examples that show how machine learning is used across industries.

Please watch the video below before our next session.https://youtu.be/9gGnTQTYNaE

In this lesson, we’ll be watching a fascinating video titled “Computer Scientist Explains Machine Learning in 5 Levels of Difficulty.” In this video, a computer scientist takes us through the concept of machine learning, breaking it down into five levels of complexity—from a basic explanation for a child all the way up to a detailed, expert-level explanation.

As you watch, you'll see how machine learning is explained in increasingly complex ways, giving you a clear progression of understanding. This approach will help you grasp the core ideas of machine learning at different levels, so whether you’re new to the topic or have some background knowledge, you’ll be able to follow along and deepen your understanding.

By the end of the video, you should have a more solid understanding of what machine learning is, how it works, and its real-world applications. Let’s get started and enjoy this insightful journey through the world of machine learning! https://youtu.be/5q87K1WaoFI

## Week 6: Supervised
As we've already learned, Machine Learning (ML) relies on vast amounts of data to uncover patterns, make predictions, and drive intelligent behavior in systems. There are two primary learning paradigms that guide how ML algorithms process and learn from data: Supervised Learning and Unsupervised Learning. These methodologies form the backbone of many AI applications, each offering distinct approaches to data analysis and problem-solving.

Supervised Learning involves training algorithms on labeled datasets, where the desired output is known. This process requires human intervention to meticulously go through the dataset and create labels that define the correct output for each data point. This approach is akin to learning with a teacher, as the labeled examples guide the model to make accurate predictions based on past examples. By providing clear guidance through these labels, the model learns to generalize from the training data to make predictions on new, unseen data.

In contrast, Unsupervised Learning deals with unlabeled data, challenging algorithms to identify hidden patterns and structures without explicit guidance. This approach allows the model to explore the data independently, discovering inherent groupings or associations within the dataset.

This reading will provide a comprehensive exploration of these two foundational approaches, highlighting their differences, strengths, and applications.  Please read: https://www.geeksforgeeks.org/supervised-unsupervised-learning/

After reading the above article, watch this video, which summarizes the two types of learning and quickly discusses example algorithms for each type.

https://youtu.be/W01tIRP_Rqs

- Linear Regression: In an earlier section, we looked at a simple example of linear regression when we used machine learning to estimate the price of a cab ride based on distance. On this page, we’ll revisit linear regression in more detail—this time applied to predicting house prices based on square footage. Linear regression is one of the most common forms of supervised learning, and it offers a clear example of how machine learning models can make predictions from labeled data.

The video below offers a deep dive into how linear regression works. It includes some mathematical detail, but don’t worry—you don’t need to understand the math itself. What’s important is that you grasp the general idea behind how this kind of model is created and how it can be used.

Linear regression is a supervised learning algorithm, which means it learns from labeled data. In the house pricing example, the training set is made up of houses where both the square footage (input) and the actual sale price (label/output) are already known. Someone has already “labeled” the prices for those houses in the dataset.

The algorithm uses this information to produce a model—in this case, a simple equation for a line:
Price = 108.35 × (Square Footage) + 60,919.44

Now that we have this model, we can make predictions about houses not in the original dataset. For example, if a new house is 2,000 square feet, we can estimate its price by plugging it into the model:
108.35 × 2000 + 60,919.44 = $277,619.44

Watch the video below to see how this process works in action.
https://youtu.be/gPfgB4ew3RY

- Logistic Regression
Linear regression is a great tool for predicting continuous outcomes, like price or temperature. But what if your data only has two outcomes—like yes or no, success or failure, clicked or didn’t click? That’s where logistic regression comes in.

On this page, we’ll introduce logistic regression with a video that explains how it works. The video does dive into some math, but don’t worry—our focus is not on memorizing formulas. Instead, pay attention to the intuition behind the model: how it lets us predict probabilities and make decisions based on binary outcomes.

What matters most is understanding when to use logistic regression. It’s not about which model is “better”—it’s about which model is suited to the type of data you have.

After watching, you’ll have a foundational sense of how logistic regression fits into the broader set of tools for data analysis and prediction.
- https://www.youtube.com/watch?v=CuvIc8C3EDI&t=2s


## Week 7: ML 2
In this section, we’ll take a closer look at Decision Trees, a popular and easy-to-understand machine learning model used in supervised learning. Decision trees work by asking a series of yes/no questions about the data, gradually narrowing down options to make a prediction, just like playing a game of “20 Questions.”

The video below walks through a simple example using an app recommendation system. In this case, the training data includes information like age and gender, along with a labeled outcome—the specific app a person downloaded. The decision tree learns patterns from this labeled data and uses them to make future predictions.

As you’ll see, the tree creates a structure where each “branch” represents a decision based on a feature (like "Is the user an adult?"), and each “leaf” leads to a predicted outcome (like recommending a particular app).

You don’t need to memorize the math or technical rules, instead, focus on understanding how the tree splits data into groups to make a prediction, and how that structure can be applied to new users.

https://youtu.be/HkyWAhr9v8g

- KNN in AI
Introduction to K-Nearest Neighbors (KNN) in AI
Welcome to our lesson on K-Nearest Neighbors (KNN), a simple but powerful machine learning algorithm commonly used for classification tasks. Let’s break down the core concepts of KNN and understand how it works step by step.

Please watch this video introducing KNN and then we will discuss it further below:
https://youtu.be/b6uHw7QW_n4

    What is KNN?
    K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It’s a simple, intuitive algorithm that makes predictions based on the closest training examples in the data. Here's how it works:

    Supervised Learning: KNN is a type of supervised learning, which means it requires labeled data to learn from. You provide the algorithm with training data (input features along with their correct output labels), and KNN uses this information to make predictions for new, unseen data.

    Classification: In a classification problem, the algorithm predicts which category (or class) a new instance belongs to based on the majority class of its neighbors.

    Regression: In a regression problem, KNN predicts a continuous value by averaging the values of its neighbors.

    How KNN Works:
    Input Data: You start with a dataset that includes labeled examples. Each example consists of several features (like age, height, weight) and an associated label (for example, "cat" or "dog" in a classification task).

    Choosing K (Number of Neighbors):
    The first step in KNN is to choose a value for K. K represents the number of nearest neighbors that will be considered when making a prediction. For example, if K = 3, the algorithm will look at the three nearest neighbors of a given data point to make its prediction.

    Distance Metric:
    KNN relies on a distance metric to find the nearest neighbors. The most common distance metric is Euclidean distance, which measures the straight-line distance between two points in space. In the case of multiple features (dimensions), it’s the distance between two points in a multi-dimensional space.

    Class Prediction (for Classification):
    Once the distances to all points in the training data are calculated, the algorithm identifies the K nearest neighbors. For classification, the algorithm assigns the most common label (the majority class) among these neighbors to the new data point.

    Example: If K=3 and two of the three nearest neighbors are labeled "cat" and one is labeled "dog," the algorithm will classify the new data point as a "cat."
    Prediction (for Regression):
    For regression tasks, instead of assigning a class label, the KNN algorithm averages the values of the K nearest neighbors to make a prediction.

    Key Considerations in KNN:
    Choosing the Right K:
    The value of K is crucial to the algorithm's performance. A small value (like K=1) can be very sensitive to noise, meaning it may overfit the data. A large value (like K=50) can smooth out predictions but might miss some important patterns in the data.

    Low K: May overfit and be sensitive to noise.
    High K: Can generalize too much and miss subtle patterns.
    Feature Scaling:
    Since KNN is based on distance, the scale of the features matters. For example, if one feature (like height) ranges from 0 to 100, and another feature (like weight) ranges from 0 to 200, the distance calculation might be dominated by the larger scale feature. Feature scaling (like normalization or standardization) is often used to ensure that all features contribute equally to the distance metric.

    Distance Metric:
    While Euclidean distance is most commonly used, other metrics, such as Manhattan distance or Minkowski distance, can also be applied, depending on the problem.

    Advantages and Disadvantages of KNN:
    Advantages:
    Simple and Intuitive: The algorithm is easy to understand and doesn’t require a lot of training.
    No Training Phase: KNN is a lazy learner, meaning it doesn’t need to build a model before making predictions. It simply memorizes the training data.
    Versatile: It can be used for both classification and regression tasks.
    Disadvantages:
    Computationally Expensive: Since KNN needs to calculate distances to all training examples at the time of prediction, it can be slow, especially for large datasets.
    Sensitive to Irrelevant Features: KNN may not perform well if there are irrelevant features in the dataset that affect the distance calculation.
    Curse of Dimensionality: As the number of features increases, the performance of KNN can degrade because the distance between points becomes less meaningful in high-dimensional spaces.
    When to Use KNN:
    When the dataset is small to medium-sized, since KNN can be slow with large datasets.
    For problems with well-defined feature spaces where the relationships between the features and target variables are clear.
    For simple problems where you don’t need to worry too much about advanced models.
    Example Use Cases for KNN:
    Image Classification: KNN can classify images based on pixel values.
    Medical Diagnosis: KNN can be used to predict whether a patient has a particular disease based on various features (e.g., age, blood pressure, etc.).
    Recommendation Systems: KNN can help recommend products based on the preferences of users with similar tastes.
    Summary:
    K-Nearest Neighbors (KNN) is a simple and powerful machine learning algorithm that classifies or predicts data based on the similarity (distance) to other data points. It's intuitive, effective for small to medium datasets, and flexible, but it can struggle with larger datasets and irrelevant features. In this course, you'll learn how to implement KNN and understand its strengths and limitations, laying the foundation for more complex algorithms in machine learning.

- In this section, we’ll explore the concept of clustering, a key technique in unsupervised learning. Unlike supervised learning, where a model learns from human-labeled examples, unsupervised learning is all about finding hidden patterns or groupings in data without any predefined categories.

The video below introduces two common clustering methods: K-means and PAM (Partitioning Around Medoids). Both of these approaches aim to group similar items together based on shared characteristics, but they do so in slightly different ways.

In the video, you’ll see how clustering can help us organize data into meaningful groups, even when we don’t know what those groups should be ahead of time. While the end of the video dives into some more technical details, don’t worry—you’re not expected to understand the math or implementation. Just focus on the core idea: clustering helps AI make sense of unlabeled data by finding natural groupings based on similarity.

Let’s take a look at how it works.
https://youtu.be/KtRLF6rAkyo


## Week 8: NN
### Part 1
Today, we’ll be diving into a crucial topic in the field of Artificial Intelligence: the difference between Machine Learning and Deep Learning. In this video from IBM, we’ll get a clear understanding of how these two technologies relate to one another, and where they differ in terms of approach, applications, and complexity.

While Machine Learning focuses on using algorithms to learn from data and make predictions or decisions, Deep Learning, which is a subset of Machine Learning, takes things a step further by simulating the human brain's neural networks to handle more complex problems, often with larger datasets.

As we explore the video, think about how each approach could be applied in the real world, from recommendation systems to autonomous vehicles.

https://youtu.be/q6kJ71tEYqM

### Part 2
You’ve already been introduced to supervised learning, where machine learning models learn from labeled examples. Now, we’re taking the next step: introducing neural networks—a powerful and flexible type of model used in many real-world AI applications.

The video below begins by briefly reviewing supervised learning, which should feel familiar. It then introduces the concept of a perceptron, which is the building block of a neural network. A perceptron is a very simple model that takes in inputs, applies weights, adds them up, and makes a decision based on the result (should I order pizza?)

While a single perceptron can only solve very basic problems, neural networks are created by connecting many perceptrons together in layers, allowing the model to recognize much more complex patterns. In this way, a neural network is essentially a group of perceptrons working together, each helping the system learn more about the data.

As you watch, don’t worry too much about the math—just focus on the core idea: neural networks learn by adjusting the connections (weights) between perceptrons based on feedback. This video will give you a helpful foundation for understanding how these networks function.

https://youtu.be/4qVRBYAdLAo

The previous video explored the basics of neural networks and how individual perceptrons function. Now, we're going to see how connecting multiple perceptrons can create powerful systems capable of complex tasks.

The video below, "Neural Networks and Deep Learning: Crash Course AI #3," delves into how combining artificial neurons forms an artificial neural network. It explains how these networks are structured with input, hidden, and output layers, and how they process information to recognize patterns, such as distinguishing between different images.

This video also introduces the concept of deep learning, which involves neural networks with multiple hidden layers. You'll learn about the significance of datasets like ImageNet in training these networks and how advancements like AlexNet have propelled the field forward.

As you watch, focus on understanding how the architecture of neural networks enables them to handle complex tasks, and consider how this builds upon the perceptron model we've previously discussed.

https://youtu.be/oV3ZY6tJiA0

### Part 3: Reinforcement learning
So far in this course, we’ve explored supervised learning, where models learn from labeled data, and unsupervised learning, where models find patterns without labels. We’ve also looked at neural networks, which can be used in both types of learning to make predictions and recognize patterns.

In this module, we’re introducing a third major type of machine learning: reinforcement learning. Unlike the previous approaches, reinforcement learning is about learning through trial and error. An agent (like a robot or game-playing character) interacts with an environment, makes decisions, and receives rewards or penalties based on its actions. Over time, the agent learns which actions lead to the best outcomes.

The video below explains reinforcement learning in a clear and high-level way, using relatable examples to show how this process works. Focus on the core idea: reinforcement learning teaches machines how to make good decisions based on feedback.

https://youtu.be/oV3ZY6tJiA0

### Part 4:
To conclude our module on neural networks, we'll watch a video that brings together the concepts we've explored. This video, titled "But what is a neural network?" by 3Blue1Brown, offers a visual and intuitive explanation of how neural networks function.

The video begins by discussing the challenge of recognizing handwritten digits—a task that's simple for humans but complex for computers. It then introduces the structure of a basic neural network, illustrating how layers of interconnected nodes (neurons) process input data to produce an output.

Through engaging animations, the video demonstrates how each layer in the network transforms the data, enabling the system to identify patterns and make decisions. It also touches on the concept of training the network using labeled examples, allowing it to improve its accuracy over time.

As you watch, focus on understanding how the network's architecture and the flow of information through its layers contribute to its ability to learn and make predictions.

https://www.youtube.com/watch?v=aircAruvnKk

## Week 9: Generative AI
### Part 1
You have already explored how traditional AI systems learn from data and make decisions. This video introduces a significant evolution in that journey: generative AI. Unlike earlier systems that focused on prediction or classification, generative models are capable of creating content such as text, images, and audio with surprising fluency and adaptability.

The first part of the video includes a review of concepts you have already encountered. As it progresses, the focus shifts to what makes generative AI unique, including its ability to understand context, respond naturally, and produce original content across different domains. These capabilities are opening up new opportunities in software development, education, creativity, and research.

As you watch, consider how generative AI might change the way we build tools, communicate ideas, and solve complex problems. 

https://youtu.be/G2fqAlgmoPo

### Part 2: GPT
This video introduces the foundational ideas behind GPT, the model that powers many modern generative AI tools. GPT stands for Generative Pretrained Transformer. It is a type of deep learning model designed to generate text by predicting what comes next in a sequence of words. " Generative" means it can create new content, "pretrained" refers to how it is first trained on large amounts of text data, and "transformer" is the specific neural network architecture it uses to understand and generate language.

The video walks through how GPT models generate language, what the term "transformer" really means, and how these models build on deep learning concepts to predict and generate coherent sequences of text.

While the video dives into some mathematical and technical details, your focus should be on grasping the core concepts. Pay attention to how the model learns to understand context, represent words as vectors, and predict what might come next in a sentence. You are not expected to master the math, but do try to follow the intuition behind how GPT works. This will give you a clearer picture of what makes tools like ChatGPT possible and how they differ from earlier types of artificial intelligence.
https://www.youtube.com/watch?v=wjZofJX0v4M

### Part 3: LLM Halluciations
This video introduces an important concept in the use of large language models: hallucinations. A hallucination occurs when an AI confidently generates information that is factually incorrect, inconsistent, or misleading. These errors may sound plausible but are often completely fabricated.

The video begins with humorous but false examples to show how natural and convincing AI-generated text can be even when it is wrong. It then defines different types of hallucinations, including factual inaccuracies, contradictions within the AI's own responses, and responses that ignore or misinterpret the prompt.

It also explains why hallucinations happen. Causes include poor training data, unclear or misleading prompts, and generation methods that prioritize creativity or fluency over accuracy. The video concludes by offering strategies to reduce hallucinations through clearer prompts, adjusting model settings like temperature, and providing multiple examples to guide the AI's behavior.

As you watch, consider how hallucinations impact the reliability of generative AI and what you can do to reduce them in your own work with these tools.
https://www.youtube.com/watch?v=cfqtFvWOfg0&t=1s

### Do Assign3.md

## Week 10: Prompt Engineering
### Part 1:
This video introduces the concept of prompt engineering and explains how it helps improve the way we interact with large language models. Prompt engineering is the process of designing clear and effective instructions, or prompts, to guide an AI model in generating useful and relevant responses. Just like giving someone detailed directions helps them complete a task accurately, a well-structured prompt helps the AI understand what you are asking for.

You will learn the basics of how large language models work and why context, token limits, and specificity are important when writing prompts. The video also explains how to avoid vague or confusing instructions, how to manage longer or multi-step requests, and how to revise prompts when the output is not quite what you expected.

As you watch, keep in mind that prompt engineering is part communication skill and part problem-solving. With practice, you will be able to make AI tools more effective and efficient for your work.
https://www.youtube.com/watch?v=LAF-lACf2QY&t=2s

### Part 2: 
Prompt engineering is the practice of crafting clear, structured input to help an AI system generate useful responses. It’s a relatively new concept, just a few years ago, the term barely existed, but it's now a foundational skill for anyone working with generative AI. Because large language models (LLMs) like ChatGPT don’t "understand" language the way humans do, they depend on patterns and structure to make sense of your request. That means how you phrase your prompt can make a big difference in the quality of what you get back.

Prompt engineering isn’t an exact science. There’s no single “correct” way to write a prompt, but there are strategies that can dramatically improve results. This page introduces some of those techniques. As you explore, keep in mind that getting better at prompt engineering takes practice, much like learning to write instructions for another person.

Watch the first video below for an overview of the fundamentals of prompt engineering, including different styles like zero-shot, few-shot, and chain-of-thought prompting. It also introduces a helpful structure you can follow when building your own prompts.
https://www.youtube.com/watch?v=sZIV7em3JA8&t=2s

Now, watch the second video for a more informal but practical look at how LLMs interpret prompts and how small changes in phrasing can lead to better results. This video also introduces principles for writing prompts for both text and image generation.

https://www.youtube.com/watch?v=bIxbpIwYTXI&t=1s

As you watch, think about how you could apply these ideas to your own work, even something as simple as asking a chatbot to help write an essay or summarize a news article. Your goal is to learn how to speak the AI’s language without needing to become a technical expert.

### Part 3: Prompt Engineering Tips
Prompt engineering is a new and evolving practice. It refers to the skill of crafting effective inputs that guide artificial intelligence models, especially large language models (LLMs), to generate useful and accurate responses. In courses like this one at Tunghai University, students will learn how to get better results from AI tools not by coding, but by thinking clearly and communicating effectively.

    There is no universal formula for writing prompts. Good prompt engineering comes from trial and error, clarity of intent, and learning a few practical techniques. This page outlines the key strategies that will help you get started.

    1. Be clear and specific
    The more specific your prompt, the easier it is for the AI to understand what you want. For example, asking “Explain climate change” may lead to a vague or overly broad answer. In contrast, saying “Write a three-paragraph explanation of how carbon emissions affect climate change, using examples relevant to Taiwan” gives the AI better direction. You should always include the task, topic, tone, and format whenever possible.

    2. Treat the AI like a new student
    Think of the AI as a student who can follow instructions very well but does not know what to do unless you tell it clearly. You should break your request into smaller parts if needed. For instance, instead of saying “Help me write a business plan,” you might begin with “Create a bullet-point list of things a new beverage stand near Tunghai University campus needs to consider before opening.” Then you can build from there. Iteration is part of the process and each revision will help you improve the output.

    3. Use the four-part structure of good prompts
    Most effective prompts share a similar structure. Below are the four common components you can include to build stronger prompts.

        a. Context
        This sets the background or perspective for the AI. You might say “You are a small business advisor in Taichung,” or “You are a student writing for a campus newsletter.” This helps the AI know what kind of voice and role to adopt.

        b. Instructions
        Give a direct and clear task. For example, you might write “Write a 100-word product description for a handmade tea set designed by local artists,” or “List three ways university students in Taiwan can use AI to help study for final exams.”

        c. Input data (optional)
        If you are giving the AI information to work with—such as a paragraph, list, or code sample—make sure to clearly include that after your instructions. This is especially helpful when asking for summaries, translations, or feedback.

        d. Output indicator
        Tell the AI what the final result should look like. For example, you could write “Respond using a table with three columns: benefit, example, and application,” or “Begin your response with the phrase ‘In summary,’ and keep the length under 200 words.”

    Example of all four parts together:
    You are a marketing student developing a campaign for a local bubble tea shop. Write a 150-word social media post that targets university students in central Taiwan. Use a friendly and casual tone. End the post with a question to encourage comments.

    4. Learn different prompting techniques
        There are several styles of prompting you will use during this course.

        Zero-shot prompting is when you give a task without examples.
        Example: “Translate the following sentence into Mandarin."

        Few-shot prompting involves giving the AI one or more examples to guide the output.
        Example: “Here are three examples of good Instagram captions. Now write one for this new product photo.”

        Chain-of-thought prompting breaks down a task into steps, helping the AI reason through more complex problems.
        Example: “First identify the problem in this paragraph. Then suggest a clearer way to explain the same idea.”

    5. Set useful constraints
    Constraints help the AI stay focused. You can limit the word count, choose the tone, exclude certain topics, or specify an output format. For instance, “Write no more than five sentences,” or “Avoid using technical language. Explain it like you are speaking to a classmate who is new to the subject.”

    6. Revise your prompts
    It is normal if your first prompt does not work perfectly. You can ask the AI to try again, revise your instructions, or simplify your request. The best results usually come after a few attempts, especially when working with creative or unfamiliar tasks.

## Week 11: AI and Data
### Part 1
Now that we have discussed how data powers AI, we will take a closer look at what data actually is, how it is organized, and why it plays a critical role in the success or failure of an AI system.



We will begin with a broad overview of different types of data, the importance of collecting and organizing it, and the role data plays in everyday technologies such as weather apps and digital banking tools.



The first video introduces these foundational ideas

.

Video 1: What is Data? (Data Fundamentals)

This video explains the basic concept of data as simple facts and figures that become useful when collected and given context. It also introduces three main types of data:

Structured data, which is highly organized in formats like tables and spreadsheets
Semi-structured data, which has some organization but allows flexibility, such as JSON files
Unstructured data, which includes content like images, videos, or audio that is more difficult to categorize
The video uses real-world examples to show how essential well-organized data is to making modern technologies function effectively.
https://www.youtube.com/watch?v=PEWMgtu-1e4&t=1s

Now that we have discussed how data powers AI, we will take a closer look at what data actually is, how it is organized, and why it plays a critical role in the success or failure of an AI system.



We will begin with a broad overview of different types of data, the importance of collecting and organizing it, and the role data plays in everyday technologies such as weather apps and digital banking tools.



The first video introduces these foundational ideas

Video 1: What is Data? (Data Fundamentals)



This video explains the basic concept of data as simple facts and figures that become useful when collected and given context. It also introduces three main types of data:

Structured data, which is highly organized in formats like tables and spreadsheets
Semi-structured data, which has some organization but allows flexibility, such as JSON files
Unstructured data, which includes content like images, videos, or audio that is more difficult to categorize
The video uses real-world examples to show how essential well-organized data is to making modern technologies function effectively.

### Part 2: 
Now that you understand the different types of data, we're going to focus on how data is used to train AI models.

The second video explains how AI developers split data into different sets to make sure the AI model learns properly without just memorizing (overfitting) or failing to learn enough (underfitting). Specifically, it introduces three critical types of datasets:

Training Set: The data the AI model uses to learn patterns.

Validation Set: A separate set of data used during training to check whether the model is learning properly without just memorizing.

Test Set: A final, unseen dataset used after training to evaluate how well the model performs on truly new data.

You’ll also hear two new terms:

Epoch: One complete pass through the full training dataset. Most models are trained over many epochs to gradually improve. We saw an earlier cartoon-like video where a "robot" was trained to walk, and each advancement took numerous training attempts.  Each attempt is an individual epoch.  

Hyperparameters: Settings that control how a model learns, such as how fast it updates during training or how complex the model is allowed to be. Hyperparameters are set by humans before training begins and can affect model performance.

Note:
The second half of this video is a coding demonstration, which is not necessary for this class. The video will stop after the core concepts are covered.

https://www.youtube.com/watch?v=Zi-0rlM4RDs

Both of these videos emphasize a key concept: data is not simply plugged into an AI system. For AI to work well, data must be carefully organized, cleaned, and managed.


Without proper data preparation, several problems can arise:

If the data is unstructured, messy, or poorly labeled, the AI model cannot learn useful patterns

If validation is skipped during training, models may overfit—performing well on training data but poorly on anything new

If models are not tested on truly new data, there is no way to measure their real-world usefulness

This leads us to an essential area of AI development: data preprocessing.

### Part 3: Preprocessing
Introduction to Data Preprocessing



Data preprocessing refers to the steps taken before a model is trained to ensure the input data is clean, reliable, and meaningful. Common preprocessing steps include:

-Cleaning incomplete or messy data by correcting typos, filling in missing values, and standardizing formats

-Splitting data into separate sets for training, validation, and testing

-Normalizing or scaling data so that different variables are on similar value ranges

-Encoding text or categories into numbers that models can process

-Preprocessing helps models learn from clear, consistent patterns rather than being misled by disorganized inputs.

-Data Preparation Matters: Real-World Consequences

The next video explores the importance of data preparation through a real-world example involving Amazon.
https://www.youtube.com/watch?v=P8ERBy91Y90

A machine learning tool built to improve hiring practices failed because it learned from biased training data. The model absorbed patterns from historical resumes that reflected social inequalities, resulting in biased and unfair candidate evaluations.

This case study introduces several important ideas:

    - Balanced datasets are essential to avoid reinforcing bias

    - Feature engineering and normalization play a key role in shaping how models understand data

    - Even well-designed algorithms can fail if the data is flawed

This example shows why data scientists spend a significant portion of their time—often as much as 80 percent—on data preparation before training even begins. Thoughtful, thorough preprocessing is not optional; it is a foundation for successful AI.

### Part 4: Fitting
Overfitting and underfitting are two common problems in machine learning that can seriously affect how well a model performs.

Overfitting occurs when a model learns the training data too well, including the noise and random fluctuations. As a result, it may perform very well on the training set but poorly on new data because it has failed to generalize.

Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data. It performs poorly even on the training data, indicating that it hasn’t learned enough.

In the video below, you’ll get an intuitive explanation using the analogy of a toddler learning to identify apples. The speaker makes these abstract ideas more concrete by showing how a model can make incorrect assumptions when it fails to recognize that not all data fits perfectly into neat categories.
https://www.youtube.com/watch?v=vdNnrJon_Vk

Now that you’ve seen the core ideas behind overfitting and underfitting, the next video takes a deeper dive, introducing a real-world-inspired cautionary tale called the “Russian tank parable.” This story highlights how overfitting can occur when a model learns from unintended patterns in the training data, leading to serious failures when deployed.


You’ll also learn about strategies used to detect and mitigate overfitting and underfitting, such as:

    - splitting data into training and validation sets
    - using early stopping during training
    - applying data augmentation techniques
    - and implementing regularization to penalize overly complex models

This video helps tie the concepts to real machine learning workflows and explains how practitioners monitor and adjust models to ensure they generalize well.

https://www.youtube.com/watch?v=2azSqdp-_EY


### Do assignment 4

## Week 12: AI wins and losses
### Part 1: Bad case studies
Artificial Intelligence systems are only as reliable as the data that fuels them. Despite the incredible capabilities AI has demonstrated, numerous failures have reminded us that poor data quality, bias, and misaligned objectives can dramatically skew performance. This page curates real-world case studies that highlight how flawed or incomplete datasets have led to serious unintended consequences. Each example offers valuable insights into how bias can emerge, how model behavior can diverge from human values, and why data integrity is critical to ethical and effective AI deployment.

Case Study 1: Algorithmic Bias in Healthcare Predictions

This video unpacks how seemingly neutral medical AI systems can reinforce systemic bias. One example centers on a high-risk care management tool that, when evaluated, was found to recommend white patients for extra services more often than Black patients with the same level of illness. Why? The model used healthcare costs as a proxy for medical need, assuming higher spending meant higher need. But systemic inequalities mean that Black patients often receive less expensive care, despite equal or greater health issues. This choice of target variable embedded structural racism into the algorithm. Other forms of bias, such as missing demographic groups in training data or misclassified labels, are equally insidious and harder to detect. This case reminds us that decisions about what to measure, how to label data, and whose data gets used are deeply consequential.
https://www.youtube.com/watch?v=7AIAdPIom_s

Case Study 2: Microsoft Tay — A Cautionary Tale in AI Training

The story of Tay, Microsoft’s AI chatbot launched in 2016, remains one of the most notorious examples of what can go wrong when data collection isn't controlled. Designed to learn from interactions on Twitter, Tay was meant to reflect how young people talk online. But lacking filters or safeguards, Tay was quickly flooded with offensive content by trolls. Within hours, it began producing racist and inflammatory tweets, forcing Microsoft to shut it down in under a day. Tay's failure wasn’t due to bad intentions or poor engineering—it was due to a data pipeline that assumed the internet would provide representative, safe, and neutral content. This case underscores how AI systems that learn from open-ended or user-generated content need boundaries and ethical guardrails to prevent harm.
https://www.youtube.com/watch?v=EO8fsKnSKEQ

Case Study 3: Tesla Autopilot and the Dangers of Incomplete Training Data

Tesla’s Autopilot system has been involved in hundreds of crashes, including fatal ones, and a growing body of evidence suggests that limitations in its training data are a key factor. In one high-profile case, a Tesla Model 3 failed to recognize an overturned truck and crashed at full speed, killing the driver. Experts analyzing the data found that the vehicle’s camera-based system had likely never been trained on similar crash scenarios: unusual shapes, lighting conditions, or obstacles not appearing in the training set led to misclassification or no detection at all. This case highlights a fundamental risk of using narrow or incomplete datasets in safety-critical AI: if the system hasn’t seen it, it may not know how to respond. Worse, the proprietary nature of Tesla’s data means the public, and even victims’ families, often lack access to the information needed to assess the system’s behavior or demand accountability.
https://www.youtube.com/watch?v=mPUGh0qAqWA

### Part 2: The good
While much attention has been paid to the risks and failures of artificial intelligence, it is equally important to highlight where AI has made remarkable contributions. This page explores examples of AI being used for good. You will see how thoughtfully applied AI has transformed healthcare, accelerated scientific discovery, and led to innovative treatments that are already changing lives.

Each of the following videos showcases a different success story.

Case Study 1: Fast MRI

This video introduces how NYU and Meta used AI to dramatically reduce MRI scan times while maintaining image quality. The result is faster, more comfortable diagnostics and increased patient access without additional cost.
https://www.youtube.com/watch?v=9ncABdfkzuU

Case Study 2: AlphaFold

Here we see how DeepMind’s AlphaFold revolutionized our understanding of protein structures, a challenge that had stalled progress for decades. This breakthrough has accelerated vaccine development, improved our understanding of diseases, and enabled entirely new approaches to bioengineering.
https://www.youtube.com/watch?v=P_fHJIYENdI

Case Study 3: AI Designed Drugs
Here is a story of a company that used AI to discover a brand-new drug for a deadly lung condition. In just four years, their AI-designed treatment moved from concept to successful human trials, showing how AI can unlock solutions in areas where traditional methods have failed.
https://www.youtube.com/watch?v=RMYkvbhOezo

## Week 13: The Problem with AI Generated Art
In this compelling TEDxBerkeley talk, professional visual artist Steven Zapata delves into the ethical and creative challenges posed by AI-generated art. Drawing from his extensive experience in commercial art and education, Zapata expresses concern over how generative AI systems are trained on vast datasets, often scraping copyrighted material without consent, credit, or compensation. He highlights the potential for AI to replicate and replace human creativity, undermining the transformative process of art-making that has been central to his personal and professional growth.​

Zapata emphasizes the importance of preserving the human element in creativity and urges for collective action to protect artists' rights. He calls for the establishment of ethical guidelines and legal frameworks to ensure that AI systems are developed responsibly, with respect for content creators. Through his talk, Zapata invites viewers to consider the future of art in an age where technology increasingly intersects with human expression.

https://www.youtube.com/watch?v=exuogrLHyxQ

### Part 2: Ethics of AI Art
In this thought-provoking TEDxNortheasternU talk, Melody Liu, a Computer Science and Business Administration graduate from Northeastern University and co-founder of the Digital Illustration Association, delves into the ethical implications of AI-generated art. Liu examines how AI challenges traditional notions of creativity, authorship, and intellectual property, raising questions about the value of human artistry in an increasingly automated world.

Liu highlights that over 15 billion images have been generated by AI, blurring the lines between human and machine-created art. She discusses the limitations of AI in understanding context and meaning, emphasizing that while AI can produce visually appealing images, it lacks the intrinsic expression and intent that human artists imbue into their work.​

The talk also addresses the impact of AI-generated art on original artists' livelihoods, as AI tools can replicate styles and techniques, potentially reducing demand for human-created art. Liu advocates for ethical considerations in AI development, suggesting that artists' consent and compensation should be prioritized when their works are used in AI training datasets.​

Through this presentation, Liu encourages a balanced approach that embraces technological innovation while preserving the integrity and value of human creativity.
https://www.youtube.com/watch?v=7Zu2aRsh9N8

### Part 3: In Defense of AI Art
In his article, Craig Boehman challenges the skepticism surrounding AI-generated art by drawing parallels to past technological advancements in the art world. He highlights how photography once faced similar criticism, with detractors claiming it would devalue traditional painting. However, over time, photography has established itself as a respected art form, coexisting with painting rather than replacing it.​

Boehman argues that AI tools, such as Midjourney and Photoshop, are merely extensions of an artist's creative process. He emphasizes that the essence of art lies in the artist's intention and connection to the work, not necessarily in the medium used. By incorporating AI into their workflow, artists can enhance their creative expression without compromising authenticity.​

The article also addresses concerns about AI's impact on the art market and copyright issues. Boehman advocates for proper regulation to protect artists' rights while acknowledging that AI is a tool that can democratize art creation.​

This discussion invites us to reconsider our definitions of art and creativity in the context of emerging technologies.

[Please read In Defense of AI Art](https://craigboehman.com/blog/in-defense-of-ai-art)

### Do assignment 5

## Week 14: AI in Business
### Part 1
Artificial Intelligence (AI) is no longer a futuristic concept; it’s a transformative force reshaping industries across the globe. In business, AI is helping companies optimize operations, improve decision-making, and deliver innovative products and services. From automating repetitive tasks to analyzing vast amounts of data for actionable insights, AI is unlocking new possibilities that were once unimaginable. This chapter explores how AI is being integrated into business strategies, examining its impact on everything from marketing and customer service to supply chain management and finance. By understanding AI’s role in business today, you’ll gain valuable insights into how this technology is driving growth, enhancing competitiveness, and revolutionizing the way we work. Let’s dive into the exciting world of AI and its potential to transform the business landscape.

In this video, "How AI Could Empower Any Business," renowned AI expert Andrew Ng explores the vast potential of artificial intelligence to transform businesses of all sizes. He emphasizes that AI is not just for tech giants—it's a tool that can be leveraged by companies across industries to improve efficiency, drive innovation, and create new opportunities. Drawing on real-world examples and his deep expertise, Ng explains how AI can be a powerful asset, enabling businesses to make smarter decisions, streamline operations, and ultimately compete in an increasingly digital world. Watch this video to gain valuable insights into how AI can empower your business, regardless of its size or industry.
https://www.youtube.com/watch?v=reUZRyXxUs4

### Part 2: AI Transforming the world
In this FT Working It video, "AI is Transforming the World of Work, Are We Ready for It?", the Financial Times explores the profound impact artificial intelligence is having on the workforce. With AI rapidly advancing, businesses are facing both exciting opportunities and significant challenges. This video dives into how AI is reshaping job roles, industries, and the very nature of work itself. Experts weigh in on the potential for AI to drive productivity, but also the questions it raises about automation, job displacement, and the skills needed for the future. Watch this insightful discussion to understand how AI is transforming the workplace and what we need to do to prepare for the changes ahead.

https://www.youtube.com/watch?v=hQX_wIW9Nh0

### Part 3: Using AI as an Entrepreneur
This video introduces a powerful and personal perspective on how artificial intelligence is reshaping the business world. It explores both the risks and opportunities AI presents, especially for entrepreneurs and small business owners. Through real stories and clear strategies, the speaker emphasizes how AI can be used not to replace humans but to enhance their value and impact. As you watch, consider how these lessons apply to your own goals and where AI might become a helpful partner in your work.
https://www.youtube.com/watch?v=stwyaUFzvEw

### Part 4: How will AI change the workforce and what does that mean for you?
In this section, we explore how generative AI tools like ChatGPT, Claude, and Gemini are reshaping not just businesses but the very nature of work. As AI technologies rapidly evolve, they are influencing productivity, job design, hiring trends, and the skills needed to thrive in the future economy.

To deepen your understanding of these trends, please read the article linked below from the Brookings Institution:

Required Reading:
Generative AI, the American Worker, and the Future of Work (Brookings)

Summary of Key Points:

Generative AI is transforming how businesses operate, particularly in knowledge and creative work.

While some jobs may be displaced, many roles will evolve, requiring new hybrid skills that combine human judgment with AI fluency.

Workers in customer service, legal services, marketing, and financial services may see some tasks augmented or replaced by AI—but not necessarily entire jobs.

The impact of AI depends largely on how businesses choose to implement these tools, and how workers and institutions adapt.

There is a growing need for proactive investment in workforce training and thoughtful workplace policies to ensure inclusive adaptation.

https://www.brookings.edu/articles/generative-ai-the-american-worker-and-the-future-of-work/

## Week 15: Increasing Data Driven Business Decisions
### Part 1
In this video, Kas Attanapola from IBM Technology breaks down how generative AI is changing the landscape of business intelligence (BI). The focus is on how AI is helping organizations turn data into action more efficiently by enabling a broader range of users—from analysts to business managers—to work with data in smarter, more accessible ways.

https://www.youtube.com/watch?v=io6JqPG80WU

### Part 2
In her TED Talk, "What Will Happen to Marketing in the Age of AI?", Jessica Apotheker explores how generative AI is set to revolutionize the marketing industry. She draws parallels to past technological advancements, such as the introduction of word processors and spreadsheets, which, while promising increased productivity, led to more complex tasks and longer work hours. Similarly, generative AI is expected to enhance marketing productivity by up to 50%, but it also brings challenges like content overload and potential loss of brand uniqueness.

Apotheker emphasizes the importance of developing a "left-AI brain"—strategically reskilling marketing teams to integrate AI tools into decision-making processes. She advises companies to build teams of marketing data scientists and engineers to predict outcomes and understand consumer behaviors. However, she also cautions against over-reliance on AI for creative tasks, as it can lead to homogenized content and stifle innovation. Protecting and nurturing human creativity within marketing teams is crucial to maintaining brand identity and differentiation in the market.

Apotheker concludes by urging marketers to assess their strengths and choose a path: specialize in creative innovation or develop data-driven, AI-augmented skills. By balancing human ingenuity with AI capabilities, marketers can navigate the evolving landscape and harness the full potential of AI in marketing.

https://www.youtube.com/watch?v=3MwMII8n1qM

### Part 3: AI and HR
AI is playing a growing role in Human Resources, helping HR professionals streamline workflows, personalize employee experiences, and make more informed decisions. This page highlights two perspectives on how AI is being used in HR today.

The first video provides a big- picture view of how AI is reshaping HR functions such as recruitment, onboarding, learning and development, performance management, and workforce planning. It emphasizes the importance of upskilling for the future and introduces examples of tools like ChatGPT for building job profiles and analyzing employee turnover.

The second video takes a practical approach, introducing specific AI tools that help HR professionals save time and improve quality. It features ChatGPT, Grammarly, Brightmine, and Fireflies.ai, offering concrete examples of how these tools are used for communication, compliance, and meeting management. The video also outlines key risks and compliance concerns to be aware of when adopting AI in HR.

https://www.youtube.com/watch?v=OK2mmINL4NY
https://www.youtube.com/watch?v=sfrBvPKwuVU

In this TEDxÉcolePolytechnique talk, Thomas Larrieu, a supply chain expert and co-founder of Altasell, delves into how artificial intelligence (AI) is set to transform global supply chains. He emphasizes the critical role of supply chains in the global economy, highlighting their often invisible yet essential presence. Larrieu illustrates the potential of AI to enhance logistics, optimize transportation, and reduce environmental impact by minimizing CO₂ emissions. He envisions a future where AI-driven systems streamline the entire process of optimizing shipments, predicting delivery times, and reducing costs while minimizing CO₂ emissions. ​

This talk is particularly relevant for professionals in logistics, supply chain management, and sustainability, as it offers insights into the integration of AI technologies to create more efficient and environmentally friendly supply chain operations.
https://youtu.be/p_70O0TdJLM

## Week 16: Personal finance
Artificial intelligence is changing how people around the world manage their money. This video introduces key ways AI is being used in personal finance, including budgeting, investment planning, debt reduction, credit improvement, and even launching small businesses. While some of the financial terms mentioned may reflect the U.S. system, the underlying ideas are widely applicable and will serve as a strong foundation for this module.

https://youtu.be/X1A_6bi7EJk

As AI becomes more integrated into the world of personal finance, one area that has seen rapid growth is the use of robo-advisors. These digital tools use algorithms to provide automated investment advice and portfolio management, offering a streamlined alternative to traditional financial advisors. In Taiwan and around the world, robo-advisors are becoming a popular choice for investors seeking lower-cost, data-driven approaches to managing their money.

### Part 2: robo advisors
In the video below, you will learn what robo-advisors are, how they work, and what their strengths and limitations might be. This will help you better understand how AI can assist with investment decisions, even if you’re just starting to think about your financial future.
https://www.youtube.com/watch?v=6cHe8IlAQe4

In this part of the module, we explore how artificial intelligence is transforming the way credit scores are calculated and used, especially through alternative credit scoring models.


### Part 3: Credit scores
Traditional credit scoring systems rely on a limited set of financial data points, such as payment history, outstanding debt, and length of credit history. While these systems work for many, they often overlook individuals who don’t fit conventional molds, such as those with irregular incomes, limited credit history, or unique financial behaviors. Alternative credit scoring aims to fill this gap by using new types of data (such as rent payments, utility bills, and mobile phone usage) and advanced machine learning techniques to build a more complete financial profile.

A key concept to keep in mind during this video is explainability. Explainability refers to how well a human can understand the decisions made by an AI system. In the context of credit scoring, explainability means giving people clear, understandable reasons why they were approved or denied, and offering guidance on how to improve their credit standing. This is especially important when AI decisions can significantly impact someone's financial future.

As you watch the video, consider how AI is used not only to assess creditworthiness but also to give consumers insight into their own financial behavior. Think about how this compares to the more opaque decisions of traditional credit bureaus. How does explainability influence fairness, trust, and accessibility in financial services?

Watch the video below and reflect on how AI is reshaping personal finance through more flexible and transparent scoring systems.
https://youtu.be/zv2SK2gaXDQ

### Part 4: Banking
Artificial intelligence is rapidly transforming the banking industry, changing how banks interact with customers, manage internal processes, and respond to risk. On this page, we will explore how AI is being used to improve five key areas of banking: customer service, personal financial assistance, fraud prevention, risk and compliance management, and global access to banking.

As you watch the video, pay special attention to the shift toward personalization and automation. Think about what it means to have AI not only assist customer service representatives, but also serve as your own personal banking assistant—learning your habits, guiding your decisions, and potentially stopping fraud before it happens.

Also consider the broader implications of this technology. How might AI make banking more inclusive and accessible to people who have been traditionally underserved by financial institutions? What new responsibilities do banks have when relying on AI to manage risk and ensure compliance?

Watch the video and reflect on the changes already happening in banking, as well as the future possibilities AI may unlock.
https://youtu.be/atrdkKbW5Uo


## Week 17: Edu
Artificial intelligence is transforming nearly every aspect of our world, and education is no exception. But with that transformation comes a deep set of questions. How do we ensure accuracy when information is generated instantly by machines? What happens when students are using AI tools more quickly and freely than their institutions can adapt? And how do we prevent a future where convenience and engagement take priority over truth and understanding


This video introduces the challenges and opportunities facing education in the age of generative AI. As you watch, consider how traditional sources of trust in learning—textbooks, teachers, and libraries—are being disrupted. Think about the implications of widespread student adoption of AI tools before formal classroom guidance is in place. Pay special attention to the speaker’s concerns about misinformation and the importance of accountability and peer review.
https://youtu.be/oV6HWzzeD-I

### Part 2: Grad school
Artificial intelligence is often presented as a tool to boost productivity, streamline research, and give students an academic edge. But as this next video shows, that promise comes with a hidden cost for many learners: anxiety, uncertainty, and even impostor syndrome.



In this talk, you will hear about how AI use among graduate students—particularly large language models like ChatGPT—is contributing to a new kind of stress. While AI has the potential to help students learn faster, find structure in their ideas, and get helpful feedback, it can also derail their process, create dependency, and lead to confusion when it gives confident but incomplete answers.



This video invites you to think critically about when AI supports learning and when it undermines it. It also connects to an ongoing theme in this course: the importance of mentorship, community, and human feedback in education. As you watch, reflect on your own experiences with AI in academic work and consider how the right use of technology depends not only on the tool but also on the support systems around it.

https://youtu.be/PoFYWH1LB3Q

### Part 3: Skills for Students
What kinds of skills will help students thrive in a world infused with artificial intelligence? As tools like ChatGPT, DALL·E, and others become everyday companions in academic, creative, and professional work, educators and students alike must learn how to use them wisely. In this video, you'll explore six key skills that prepare learners to make the most of AI: seeking feedback, visualizing ideas, brainstorming, summarizing complex information, deepening understanding through debate, and using AI as a learning partner.

As you watch, think about which of these skills you already practice and which might be worth developing. Also consider how you might apply them not only in school, but in your future work or personal life.

https://youtu.be/Xiw95yjp_I8

### Part 4: Using AI responsibly in college
As AI becomes a more common tool in education, it is essential that students and instructors alike understand what responsible use looks like. In this video, Dr. Brian Doak breaks down the ethical boundaries around using AI in academic settings. He explains why citation matters, how scholarly communities value honesty and transparency, and what kinds of AI use are clearly acceptable, what might fall into a gray area, and what constitutes academic dishonesty.  We'll use this video to spark a larger conversation about valid uses of AI by college students:

https://youtu.be/VR9X9kRdgbk